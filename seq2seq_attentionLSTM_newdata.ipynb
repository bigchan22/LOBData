{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b9e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from numpy import load\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513b0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbrnlist1=[(5,194),(2,155),(12,100),(17,29),(42,1),(44,1),(50,92),(2,83),(4,10118),(8,298)]\n",
    "mbrnlist2=[(5,194),(12,100),(2,155),(17,29),(42,1),(44,1),(2,83),(4,10118),(4,9997),(50,91)]\n",
    "\n",
    "mbrnlist=mbrnlist1+mbrnlist2\n",
    "mbrnlist=set(mbrnlist)\n",
    "mbrnlist=list(mbrnlist)\n",
    "\n",
    "# load array\n",
    "MBR_NO,BRN_NO=mbrnlist[11]\n",
    "# print the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8564e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_train = load('Data0930_'+str(MBR_NO)+'_'+str(BRN_NO)+'norm_train.npy',allow_pickle=True)\n",
    "Data_test =  load('Data0930_'+str(MBR_NO)+'_'+str(BRN_NO)+'norm_test.npy',allow_pickle=True)\n",
    "\n",
    "\n",
    "Xdata=[]\n",
    "Ydata=[]\n",
    "Xtrain_data=[]\n",
    "Ytrain_data=[]\n",
    "Xtest_data=[]\n",
    "Ytest_data=[]\n",
    "\n",
    "\n",
    "\n",
    "for idx in range(len(Data_train)//39):\n",
    "    if(np.isinf(Data_train[39*idx:39*(idx+1)][:,:].tolist()).any()):\n",
    "        print(np.isinf(Data_train[39*idx:39*(idx+1)][:,:].tolist()).any())\n",
    "        continue\n",
    "    Xtrain_data.append(Data_train[39*idx:39*(idx+1)][:,:].tolist())\n",
    "    Ytrain_data.append(Data_train[39*idx:39*(idx+1)][:,-1].tolist())\n",
    "for idx in range(len(Data_test)//39):\n",
    "    if(np.isinf(Data_test[39*idx:39*(idx+1)][:,:].tolist()).any()):\n",
    "        print(np.isinf(Data_test[39*idx:39*(idx+1)][:,:].tolist()).any())\n",
    "        continue\n",
    "    Xtest_data.append(Data_test[39*idx:39*(idx+1)][:,:].tolist())\n",
    "    Ytest_data.append(Data_test[39*idx:39*(idx+1)][:,-1].tolist())\n",
    "    \n",
    "Xtrain_data=np.vstack(Xtrain_data)\n",
    "Ytrain_data=np.vstack(Ytrain_data)\n",
    "\n",
    "Xtrain_data=torch.FloatTensor(Xtrain_data)\n",
    "Ytrain_data=torch.IntTensor(Ytrain_data)\n",
    "Ytrain_data=Ytrain_data.view(-1)\n",
    "\n",
    "\n",
    "Xtest_data=np.vstack(Xtest_data)\n",
    "Ytest_data=np.vstack(Ytest_data)\n",
    "Xtest_data=torch.FloatTensor(Xtest_data)\n",
    "Ytest_data=torch.IntTensor(Ytest_data)\n",
    "Ytest_data=Ytest_data.view(-1)\n",
    "\n",
    "Ytrain_data=2*(Ytrain_data>0).long()+(Ytrain_data==0).long()\n",
    "Ytest_data=2*(Ytest_data>0).long()+(Ytest_data==0).long()\n",
    "Ytrain_data=Ytrain_data.T\n",
    "Ytest_data=Ytest_data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b37bc1a",
   "metadata": {},
   "source": [
    "Ydata=(torch.stack([Ydata>0,Ydata==0, Ydata<0],axis=0)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358866a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce44368a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([74100, 7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88194fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a8d958b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.5800e+04,  3.8780e-01,  1.7981e-01,  ...,  2.9070e-02,\n",
       "         -9.6899e-03,  0.0000e+00],\n",
       "        [ 2.5800e+04,  6.1143e-01,  2.2340e-01,  ...,  2.9070e-02,\n",
       "         -9.6899e-03,  0.0000e+00],\n",
       "        [ 2.5800e+04,  7.1929e-01,  2.2576e-01,  ...,  2.9070e-02,\n",
       "         -9.6899e-03,  0.0000e+00],\n",
       "        ...,\n",
       "        [ 2.8250e+03,  2.8344e-01,  1.0433e-01,  ...,  3.8938e-02,\n",
       "         -3.5398e-03,  0.0000e+00],\n",
       "        [ 2.8250e+03,  2.7932e-01,  1.0208e-01,  ...,  3.8938e-02,\n",
       "         -3.5398e-03,  0.0000e+00],\n",
       "        [ 2.8250e+03,  2.5743e-01,  1.1438e-01,  ...,  3.8938e-02,\n",
       "         -3.5398e-03,  0.0000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6fbe991",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "def batchify(data, bsz,bptt):\n",
    "    # 데이터셋을 bsz 파트들로 나눕니다.\n",
    "    if(data.size(0)%(bsz*bptt)!=0):\n",
    "        \n",
    "#         print(data.size(0),bsz,bptt)\n",
    "        \n",
    "        data=data.view(-1,bptt,data.size(1)).transpose(0,1).contiguous()\n",
    "        return data.to(device)\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # 깔끔하게 나누어 떨어지지 않는 추가적인 부분(나머지들) 은 잘라냅니다.\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # 데이터에 대하여 bsz 배치들로 동등하게 나눕니다.\n",
    "    data = data.view(bsz, -1,data.size(1)).transpose(0,1).contiguous()\n",
    "    return data.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7796df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bptt = 39\n",
    "def get_batch(source, i,bs):\n",
    "    seq_len = min(bptt*bs, len(source)  - i)\n",
    "    data = source[i:i+seq_len]\n",
    "#     if(seq_len!=bptt*bs):\n",
    "#         print(seq_len)\n",
    "    target = source[i:i+seq_len].reshape(-1)\n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba5f8929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "INPUT_SIZE=Xtrain_data.shape[1]\n",
    "EMB_SIZE=128\n",
    "HIDDEN_SIZE=128\n",
    "NUM_LAYERS=6\n",
    "PROJ_SIZE=3\n",
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a84822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def batch_matmul(seq, weight, nonlinearity=''):\n",
    "    s = None\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s = torch.tanh(_s)\n",
    "        _s = _s.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers,proj_size, dropout=0.5, tie_weights=False, attention=True,\n",
    "                 attention_width=3, cuda=True):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "#         self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.encoder = nn.Linear(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError(\"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        if attention:\n",
    "            self.decoder = nn.Linear(nhid, ntoken)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(nhid, ntoken)\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "        if attention:\n",
    "            self.AttentionLayer = AttentionLayer(cuda,nhid)\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.attention = attention\n",
    "        self.attention_width = attention_width\n",
    "        self.generator = nn.Linear(nhid, proj_size)\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print(\"input size:\",input.size())\n",
    "#         emb = self.drop(self.encoder(input))\n",
    "        emb=self.encoder(input)\n",
    "        #print(\"emb size:\",emb.size())\n",
    "        output, _ = self.rnn(emb)\n",
    "        #print(\"rnn output\",output.size())\n",
    "        if self.attention:\n",
    "            output = self.AttentionLayer.forward(output, self.attention_width)\n",
    "        return self.generator(output)\n",
    "        #\n",
    "#         output = self.drop(output)\n",
    "#         decoded = self.decoder(output.view(output.size(0) * output.size(1), output.size(2)))\n",
    "#         return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n",
    "                    Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))\n",
    "        else:\n",
    "            return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Implements an Attention Layer\"\"\"\n",
    "\n",
    "    def __init__(self, cuda, nhid):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.nhid = nhid\n",
    "        self.weight_W = nn.Parameter(torch.Tensor(nhid,nhid))\n",
    "        self.weight_proj = nn.Parameter(torch.Tensor(nhid, 1))\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.weight_W.data.uniform_(-0.1, 0.1)\n",
    "        self.weight_proj.data.uniform_(-0.1,0.1)\n",
    "        self.cuda = cuda\n",
    "\n",
    "    def forward(self, inputs, attention_width=3):\n",
    "        results = None\n",
    "        for i in range(inputs.size(0)):\n",
    "            if(i<attention_width):\n",
    "                output = inputs[i]\n",
    "                output = output.unsqueeze(0)\n",
    "\n",
    "            else:\n",
    "                lb = i - attention_width\n",
    "                if(lb<0):\n",
    "                    lb = 0\n",
    "                selector = torch.from_numpy(np.array(np.arange(lb, i)))\n",
    "                if self.cuda:\n",
    "                    selector = Variable(selector).cuda()\n",
    "                else:\n",
    "                    selector = Variable(selector)\n",
    "                vec = torch.index_select(inputs, 0, selector)\n",
    "                u = batch_matmul(vec, self.weight_W, nonlinearity='tanh')\n",
    "                a = batch_matmul(u, self.weight_proj)\n",
    "                a = self.softmax(a)\n",
    "                output = None\n",
    "                for i in range(vec.size(0)):\n",
    "                    h_i = vec[i]\n",
    "                    a_i = a[i].unsqueeze(1).expand_as(h_i)\n",
    "                    h_i = a_i * h_i\n",
    "                    h_i = h_i.unsqueeze(0)\n",
    "                    if(output is None):\n",
    "                        output = h_i\n",
    "                    else:\n",
    "                        output = torch.cat((output,h_i),0)\n",
    "                output = torch.sum(output,0)\n",
    "                output = output.unsqueeze(0)\n",
    "\n",
    "            if(results is None):\n",
    "                results = output\n",
    "\n",
    "            else:\n",
    "                results = torch.cat((results,output),0)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "504d3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "\n",
    "#lstm=Seq2SeqLSTM(input_size=INPUT_SIZE,hidden_size=HIDDEN_SIZE,num_layers=NUM_LAYERS,emb_size=EMB_SIZE,proj_size=PROJ_SIZE)\n",
    "\n",
    "#lstm = RNNmodel(rnn_type='LSTM',ntoken=INPUT_SIZE,ninp=INPUT_SIZE,nhid=HIDDEN_SIZE,nlayers=NUM_LAYERS,emb_size=EMB_SIZE,proj_size=PROJ_SIZE)\n",
    "lstm = RNNModel(rnn_type='LSTM',ntoken=INPUT_SIZE,ninp=EMB_SIZE,nhid=HIDDEN_SIZE,nlayers=NUM_LAYERS,proj_size=PROJ_SIZE,\n",
    "                attention_width=38)\n",
    "# for p in lstm.parameters():\n",
    "#     if p.dim() > 1:\n",
    "#         nn.init.xavier_uniform_(p)\n",
    "\n",
    "lstm = lstm.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=1e-3, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b13155b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Ytest_data==2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc53878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    total=0\n",
    "    correct=0\n",
    "    correct1=0\n",
    "    tot1=0\n",
    "    correct2=0\n",
    "    tot2=0\n",
    "    correct0=0\n",
    "    tot0=0\n",
    "    conf00=0\n",
    "    conf01=0\n",
    "    conf02=0\n",
    "    conf10=0\n",
    "    conf11=0\n",
    "    conf12=0\n",
    "    conf20=0\n",
    "    conf21=0\n",
    "    conf22=0\n",
    "    \n",
    "    \n",
    "#     for batch, i in enumerate(range(0, Xtrain_data.size(0) - 1, BATCH_SIZE*bptt)):\n",
    "    i=0\n",
    "    while(i<Xtest_data.size(0)):\n",
    "        data, org_targets = get_batch(Xtrain_data, i,BATCH_SIZE)\n",
    "        if (data.isnan().any() or data.isinf().any()):\n",
    "#            print(data)\n",
    "            continue\n",
    "#         _,targets = get_batch(Ytrain_data,i)\n",
    "        targets,_ = get_batch(Ytrain_data,i,BATCH_SIZE)\n",
    "#         src = src.to(DEVICE)\n",
    "#         tgt = tgt.to(DEVICE)\n",
    "        targets=torch.unsqueeze(targets,1)\n",
    "        src=batchify(data,BATCH_SIZE,bptt)\n",
    "        tgt=batchify(targets,BATCH_SIZE,bptt)\n",
    "        src=src[:-1]\n",
    "        tgt=tgt[1:]\n",
    "        logits = model(src)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "        _,predicted=torch.max(logits,-1)\n",
    "        correct += (tgt.squeeze() == predicted).sum().item()\n",
    "        total+=len(predicted)*BATCH_SIZE\n",
    "        tot0+=(0== tgt.squeeze()).sum().item()\n",
    "        tot1+=(1== tgt.squeeze()).sum().item()\n",
    "        tot2+=(2== tgt.squeeze()).sum().item()\n",
    "        correct0+=((0== predicted) &(0==tgt.squeeze())).sum().item()\n",
    "        correct1+=((1== predicted)&(1==tgt.squeeze())).sum().item()\n",
    "        correct2+=((2== predicted)&(2==tgt.squeeze())).sum().item()\n",
    "        \n",
    "        conf00+=((0== predicted) &(0==tgt.squeeze())).sum().item()\n",
    "        conf01+=((0== predicted)&(1==tgt.squeeze())).sum().item()\n",
    "        conf02+=((0== predicted)&(2==tgt.squeeze())).sum().item()\n",
    "        conf10+=((1== predicted) &(0==tgt.squeeze())).sum().item()\n",
    "        conf11+=((1== predicted)&(1==tgt.squeeze())).sum().item()\n",
    "        conf12+=((1== predicted)&(2==tgt.squeeze())).sum().item()\n",
    "        conf20+=((2== predicted) &(0==tgt.squeeze())).sum().item()\n",
    "        conf21+=((2== predicted)&(1==tgt.squeeze())).sum().item()\n",
    "        conf22+=((2== predicted)&(2==tgt.squeeze())).sum().item()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        i+=targets.size()[0]\n",
    "\n",
    "    tp0=conf00\n",
    "    fp0=conf01+conf02\n",
    "    fn0=conf10+conf20\n",
    "    if(tp0+fp0==0):\n",
    "        prec0=0\n",
    "    else:\n",
    "        prec0=tp0/(tp0+fp0)\n",
    "    if(tp0+fn0==0):\n",
    "        reca0=0\n",
    "    else:\n",
    "        reca0=tp0/(tp0+fn0)\n",
    "    \n",
    "    tp1=conf11\n",
    "    fp1=conf10+conf12\n",
    "    fn1=conf01+conf21\n",
    "    \n",
    "    if(tp1+fp1==0):\n",
    "        prec1=0\n",
    "    else:\n",
    "        prec1=tp1/(tp1+fp1)\n",
    "    if(tp1+fn1==0):\n",
    "        reca1=0\n",
    "    else:\n",
    "        reca1=tp1/(tp1+fn1)\n",
    "    \n",
    "    tp2=conf22\n",
    "    fp2=conf20+conf21\n",
    "    fn2=conf02+conf12\n",
    "    \n",
    "    if(tp2+fp2==0):\n",
    "        prec2=0\n",
    "    else:\n",
    "        prec2=tp2/(tp2+fp2)\n",
    "    if(tp2+fn2==0):\n",
    "        reca2=0\n",
    "    else:\n",
    "        reca2=tp2/(tp2+fn2)\n",
    "        \n",
    "    prec=(prec0+prec1+prec2)/3\n",
    "    reca=(reca0+reca1+reca2)/3\n",
    "    f1sc=2*(prec*reca)/(prec+reca)\n",
    "    print(total)\n",
    "    print(correct)\n",
    "    print(\"Acc:\",correct/total)            \n",
    "    print(\"Prec\",prec)\n",
    "    print(\"Recall\",reca)\n",
    "    print(\"F1\",f1sc) \n",
    "    return losses / Xtrain_data.size(0),[conf00,conf01,conf02,conf10,conf11,conf12,conf20,conf21,conf22]\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    total=0\n",
    "    correct=0\n",
    "    correct1=0\n",
    "    tot1=0\n",
    "    correct2=0\n",
    "    tot2=0\n",
    "    correct0=0\n",
    "    tot0=0\n",
    "    conf00=0\n",
    "    conf01=0\n",
    "    conf02=0\n",
    "    conf10=0\n",
    "    conf11=0\n",
    "    conf12=0\n",
    "    conf20=0\n",
    "    conf21=0\n",
    "    conf22=0\n",
    "    stime=time.time()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        i=0\n",
    "        while(i<Xtest_data.size(0)):\n",
    "            data, org_targets = get_batch(Xtest_data, i,BATCH_SIZE)\n",
    "            if (data.isnan().any() or data.isinf().any()):\n",
    "                print(data)\n",
    "                continue\n",
    "            targets,_ = get_batch(Ytest_data,i,BATCH_SIZE)\n",
    "            targets=torch.unsqueeze(targets,1)\n",
    "            src=batchify(data,BATCH_SIZE,bptt)\n",
    "            tgt=batchify(targets,BATCH_SIZE,bptt)\n",
    "#             print(src.shape)\n",
    "            if(src.shape[1]!=BATCH_SIZE):\n",
    "                \n",
    "                break\n",
    "#             print(tgt.shape)\n",
    "            src=src[:-1]\n",
    "            tgt=tgt[1:]\n",
    "\n",
    "\n",
    "            logits = model(src)\n",
    "#            print(logits.reshape(-1, logits.shape[-1]).shape,'AA')\n",
    "#            print(tgt.reshape(-1).shape,'BB')\n",
    "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1))\n",
    "            if(loss.isnan()):\n",
    "                print(src,tgt_input)\n",
    "                break\n",
    "            losses += loss.item()\n",
    "            _,predicted=torch.max(logits,-1)\n",
    "\n",
    "            \n",
    "            correct += (tgt.squeeze() == predicted).sum().item()\n",
    "            total+=len(predicted)*BATCH_SIZE\n",
    "            tot0+=(0== tgt.squeeze()).sum().item()\n",
    "            tot1+=(1== tgt.squeeze()).sum().item()\n",
    "            tot2+=(2== tgt.squeeze()).sum().item()\n",
    "            correct0+=((0== predicted) &(0==tgt.squeeze())).sum().item()\n",
    "            correct1+=((1== predicted)&(1==tgt.squeeze())).sum().item()\n",
    "            correct2+=((2== predicted)&(2==tgt.squeeze())).sum().item()\n",
    "            \n",
    "            conf00+=((0== predicted) &(0==tgt.squeeze())).sum().item()\n",
    "            conf01+=((0== predicted)&(1==tgt.squeeze())).sum().item()\n",
    "            conf02+=((0== predicted)&(2==tgt.squeeze())).sum().item()\n",
    "            conf10+=((1== predicted) &(0==tgt.squeeze())).sum().item()\n",
    "            conf11+=((1== predicted)&(1==tgt.squeeze())).sum().item()\n",
    "            conf12+=((1== predicted)&(2==tgt.squeeze())).sum().item()\n",
    "            conf20+=((2== predicted) &(0==tgt.squeeze())).sum().item()\n",
    "            conf21+=((2== predicted)&(1==tgt.squeeze())).sum().item()\n",
    "            conf22+=((2== predicted)&(2==tgt.squeeze())).sum().item()\n",
    "\n",
    "            \n",
    "            i+=targets.size()[0]\n",
    "    etime=time.time()\n",
    "    print(\"Time elapsed\",etime-stime)\n",
    "#     print(total,tot0,tot1,tot2)\n",
    "#     print(correct)\n",
    "    tp0=conf00\n",
    "    fp0=conf01+conf02\n",
    "    fn0=conf10+conf20\n",
    "    if(tp0+fp0==0):\n",
    "        prec0=0\n",
    "    else:\n",
    "        prec0=tp0/(tp0+fp0)\n",
    "    if(tp0+fn0==0):\n",
    "        reca0=0\n",
    "    else:\n",
    "        reca0=tp0/(tp0+fn0)\n",
    "    \n",
    "    tp1=conf11\n",
    "    fp1=conf10+conf12\n",
    "    fn1=conf01+conf21\n",
    "    \n",
    "    if(tp1+fp1==0):\n",
    "        prec1=0\n",
    "    else:\n",
    "        prec1=tp1/(tp1+fp1)\n",
    "    if(tp1+fn1==0):\n",
    "        reca1=0\n",
    "    else:\n",
    "        reca1=tp1/(tp1+fn1)\n",
    "    \n",
    "    tp2=conf22\n",
    "    fp2=conf20+conf21\n",
    "    fn2=conf02+conf12\n",
    "    \n",
    "    if(tp2+fp2==0):\n",
    "        prec2=0\n",
    "    else:\n",
    "        prec2=tp2/(tp2+fp2)\n",
    "    if(tp2+fn2==0):\n",
    "        reca2=0\n",
    "    else:\n",
    "        reca2=tp2/(tp2+fn2)\n",
    "    \n",
    "    prec=(prec0+prec1+prec2)/3\n",
    "    reca=(reca0+reca1+reca2)/3\n",
    "    if(prec+reca==0):\n",
    "        f1sc=0\n",
    "    else:\n",
    "        f1sc=2*(prec*reca)/(prec+reca)\n",
    "    print(total)\n",
    "    print(correct)\n",
    "    print(\"Acc:\",correct/total)            \n",
    "    print(\"Prec\",prec)\n",
    "    print(\"Recall\",reca)\n",
    "    print(\"F1\",f1sc)\n",
    "    return losses / Xtest_data.size(0),correct/total,prec,reca,f1sc,[conf00,conf01,conf02,conf10,conf11,conf12,conf20,conf21,conf22]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb335f",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    for batch, i in enumerate(range(0, Xtrain_data.size(0) - 1, BATCH_SIZE*bptt)):\n",
    "        data, org_targets = get_batch(Xtrain_data, i,BATCH_SIZE)\n",
    "        if (data.isnan().any() or data.isinf().any()):\n",
    "#            print(data)\n",
    "            continue\n",
    "#         _,targets = get_batch(Ytrain_data,i)\n",
    "        targets,_ = get_batch(Ytrain_data,i,BATCH_SIZE)\n",
    "#         src = src.to(DEVICE)\n",
    "#         tgt = tgt.to(DEVICE)\n",
    "        targets=torch.unsqueeze(targets,1)\n",
    "        src=batchify(data,BATCH_SIZE)\n",
    "        tgt=batchify(targets,BATCH_SIZE)\n",
    "        src=src[:-1]\n",
    "        tgt=tgt[1:]\n",
    "        logits = model(src)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "    return losses / Xtrain_data.size(0)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    total=0\n",
    "    correct=0\n",
    "    correct1=0\n",
    "    tot1=0\n",
    "    correct2=0\n",
    "    tot2=0\n",
    "    correct0=0\n",
    "    tot0=0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, Xtest_data.size(0) - 1, bptt):\n",
    "\n",
    "            data, org_targets = get_batch(Xtest_data, i,BATCH_SIZE)\n",
    "            if (data.isnan().any() or data.isinf().any()):\n",
    "#            print(data)\n",
    "                continue\n",
    "            targets,_ = get_batch(Ytest_data,i,BATCH_SIZE)\n",
    "            targets=torch.unsqueeze(targets,1)\n",
    "            src=batchify(data,BATCH_SIZE)\n",
    "            tgt=batchify(targets,BATCH_SIZE)\n",
    "#             print(src.size())\n",
    "#             print(tgt.size())\n",
    "            logits = model(src)\n",
    "#             print(logits)\n",
    "#             print(logits.size())\n",
    "\n",
    "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1))\n",
    "            if(loss.isnan()):\n",
    "                print(src,tgt_input)\n",
    "                break\n",
    "#            print(loss)\n",
    "            losses += loss.item()\n",
    "#             output_flat = logits.view(-1)\n",
    "            _,predicted=torch.max(logits,-1)\n",
    "#             print(predicted)\n",
    "#             print(logits.size())\n",
    "#             print(tgt_out.size())\n",
    "            \n",
    "            correct += (tgt.squeeze() == predicted).sum().item()\n",
    "            total+=len(predicted)*BATCH_SIZE\n",
    "            tot0+=(0== tgt.squeeze()).sum().item()\n",
    "            tot1+=(1== tgt.squeeze()).sum().item()\n",
    "            tot2+=(2== tgt.squeeze()).sum().item()\n",
    "            correct0+=((0== predicted) &(0==tgt.squeeze())).sum().item()\n",
    "            correct1+=((1== predicted)&(1==tgt.squeeze())).sum().item()\n",
    "            correct2+=((2== predicted)&(2==tgt.squeeze())).sum().item()\n",
    "            \n",
    "    print(total)\n",
    "    print(correct)\n",
    "    print(\"Acc:\",correct/total*100)            \n",
    "    print(\"Acc0:\",correct0/tot0*100,correct0,tot0)     \n",
    "    print(\"Acc1:\",correct1/tot1*100,correct1,tot1)     \n",
    "    print(\"Acc2:\",correct2/tot2*100,correct2,tot2)     \n",
    "    return losses / Xtest_data.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "823097d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 0.03901362419128418\n",
      "7296\n",
      "667\n",
      "Acc: 0.09141995614035088\n",
      "Prec 0.030473318713450295\n",
      "Recall 0.3333333333333333\n",
      "F1 0.05584160073674077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0008486893085333017,\n",
       " 0.09141995614035088,\n",
       " 0.030473318713450295,\n",
       " 0.3333333333333333,\n",
       " 0.05584160073674077,\n",
       " [0, 0, 0, 0, 0, 0, 1373, 5256, 667])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e4c7527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8512\n",
      "5184\n",
      "Acc: 0.6090225563909775\n",
      "Prec 0.2562345014738221\n",
      "Recall 0.35199573507275467\n",
      "F1 0.29657667862545356\n",
      "Time elapsed 0.05794119834899902\n",
      "7296\n",
      "5256\n",
      "Acc: 0.7203947368421053\n",
      "Prec 0.24013157894736845\n",
      "Recall 0.3333333333333333\n",
      "F1 0.27915869980879543\n",
      "Epoch: 1, Train loss: 0.000, Val loss: 0.001, Epoch time = 0.224s\n",
      "8512\n",
      "5979\n",
      "Acc: 0.7024201127819549\n",
      "Prec 0.23414003759398497\n",
      "Recall 0.3333333333333333\n",
      "F1 0.2750672831412601\n",
      "Time elapsed 0.05960440635681152\n",
      "7296\n",
      "5256\n",
      "Acc: 0.7203947368421053\n",
      "Prec 0.24013157894736845\n",
      "Recall 0.3333333333333333\n",
      "F1 0.27915869980879543\n",
      "Epoch: 2, Train loss: 0.000, Val loss: 0.001, Epoch time = 0.194s\n",
      "8512\n",
      "5979\n",
      "Acc: 0.7024201127819549\n",
      "Prec 0.23414003759398497\n",
      "Recall 0.3333333333333333\n",
      "F1 0.2750672831412601\n",
      "Time elapsed 0.044858455657958984\n",
      "7296\n",
      "5256\n",
      "Acc: 0.7203947368421053\n",
      "Prec 0.24013157894736845\n",
      "Recall 0.3333333333333333\n",
      "F1 0.27915869980879543\n",
      "Epoch: 3, Train loss: 0.000, Val loss: 0.001, Epoch time = 0.192s\n",
      "8512\n",
      "5979\n",
      "Acc: 0.7024201127819549\n",
      "Prec 0.23414003759398497\n",
      "Recall 0.3333333333333333\n",
      "F1 0.2750672831412601\n",
      "Time elapsed 0.054680824279785156\n",
      "7296\n",
      "5256\n",
      "Acc: 0.7203947368421053\n",
      "Prec 0.24013157894736845\n",
      "Recall 0.3333333333333333\n",
      "F1 0.27915869980879543\n",
      "Epoch: 4, Train loss: 0.000, Val loss: 0.001, Epoch time = 0.198s\n",
      "8512\n",
      "5987\n",
      "Acc: 0.7033599624060151\n",
      "Prec 0.5071310683591591\n",
      "Recall 0.33468405241128757\n",
      "F1 0.4032445530481083\n",
      "Time elapsed 0.05778694152832031\n",
      "7296\n",
      "5422\n",
      "Acc: 0.7431469298245614\n",
      "Prec 0.5123633484701182\n",
      "Recall 0.37668345969211986\n",
      "F1 0.43417016280641973\n",
      "Epoch: 5, Train loss: 0.000, Val loss: 0.001, Epoch time = 0.189s\n",
      "8512\n",
      "6651\n",
      "Acc: 0.7813674812030075\n",
      "Prec 0.535838957541016\n",
      "Recall 0.4508146852787695\n",
      "F1 0.4896633641641139\n",
      "Time elapsed 0.040702104568481445\n",
      "7296\n",
      "5618\n",
      "Acc: 0.7700109649122807\n",
      "Prec 0.48534342275268766\n",
      "Recall 0.43413250323426017\n",
      "F1 0.4583118472008783\n",
      "Epoch: 6, Train loss: 0.000, Val loss: 0.001, Epoch time = 0.193s\n",
      "8512\n",
      "6705\n",
      "Acc: 0.7877114661654135\n",
      "Prec 0.5091773500358173\n",
      "Recall 0.47905667962576004\n",
      "F1 0.49365798652437526\n",
      "Time elapsed 0.05960845947265625\n",
      "7296\n",
      "5609\n",
      "Acc: 0.7687774122807017\n",
      "Prec 0.49141083266930535\n",
      "Recall 0.4283603510922949\n",
      "F1 0.457724530903202\n",
      "Epoch: 7, Train loss: 0.000, Val loss: 0.001, Epoch time = 0.191s\n",
      "8512\n",
      "6836\n",
      "Acc: 0.8031015037593985\n",
      "Prec 0.5354958324481524\n",
      "Recall 0.4887725887300347\n",
      "F1 0.5110685419331021\n",
      "Time elapsed 0.04124784469604492\n",
      "7296\n",
      "5665\n",
      "Acc: 0.776452850877193\n",
      "Prec 0.5155965054046469\n",
      "Recall 0.43783065483745925\n",
      "F1 0.47354211209149616\n",
      "Epoch: 8, Train loss: 0.000, Val loss: 0.001, Epoch time = 0.207s\n",
      "8512\n",
      "6868\n",
      "Acc: 0.8068609022556391\n",
      "Prec 0.5344673698354225\n",
      "Recall 0.4963869872103542\n",
      "F1 0.5147238223547933\n",
      "Time elapsed 0.05804944038391113\n",
      "7296\n",
      "5634\n",
      "Acc: 0.772203947368421\n",
      "Prec 0.5118119029257953\n",
      "Recall 0.4344297854210155\n",
      "F1 0.46995675185784425\n",
      "Epoch: 9, Train loss: 0.000, Val loss: 0.001, Epoch time = 0.198s\n",
      "8512\n",
      "6703\n",
      "Acc: 0.7874765037593985\n",
      "Prec 0.5309258539114654\n",
      "Recall 0.464067665489943\n",
      "F1 0.4952505051915809\n",
      "Time elapsed 0.05838608741760254\n",
      "7296\n",
      "5506\n",
      "Acc: 0.7546600877192983\n",
      "Prec 0.5121581347638652\n",
      "Recall 0.3977941901933461\n",
      "F1 0.44778946079160037\n",
      "Epoch: 10, Train loss: 0.000, Val loss: 0.001, Epoch time = 0.196s\n",
      "8512\n",
      "6833\n",
      "Acc: 0.802749060150376\n",
      "Prec 0.5375776445434646\n",
      "Recall 0.48790167053667366\n",
      "F1 0.5115364629181812\n",
      "Time elapsed 0.048555850982666016\n",
      "7296\n",
      "5622\n",
      "Acc: 0.7705592105263158\n",
      "Prec 0.4974875583649084\n",
      "Recall 0.43295131925668\n",
      "F1 0.4629812874080492\n",
      "Epoch: 11, Train loss: 0.000, Val loss: 0.001, Epoch time = 0.189s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-546417b272e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreca\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1sc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f4707ecf254d>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-7bed17a585c3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0memb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m#print(\"emb size:\",emb.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;31m#print(\"rnn output\",output.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 680\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 1000\n",
    "best_val_loss=100000000\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss,_ = train_epoch(lstm, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss,acc,prec,reca,f1sc,confusion = evaluate(lstm)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_confusion=confusion\n",
    "        best_acc=acc\n",
    "        best_prec=prec\n",
    "        best_reca=reca\n",
    "        best_f1sc=f1sc\n",
    "        best_model = lstm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eb335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "date_time = now.strftime(\"%m_%d_%Y\")\n",
    "PATH='best_model_seq_attLSTM_'+date_time+str(MBR_NO)+'_'+str(BRN_NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30eddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), PATH)\n",
    "file_name='result_attLSTM_norm'+date_time+'.txt'\n",
    "text_to_append=PATH+'\\t'+\"Acc:\"+str(best_acc)+'\\t'+\"prec:\"+str(best_prec)+'\\t'+\"recall:\"+str(best_reca)+'\\t'+\"f1sc:\"+str(best_f1sc)\n",
    "print(text_to_append)\n",
    "with open(file_name, \"a+\") as file_object:\n",
    "    # Move read cursor to the start of file.\n",
    "    file_object.seek(0)\n",
    "    # If file is not empty then append '\\n'\n",
    "    data = file_object.read(100)\n",
    "    if len(data) > 0:\n",
    "        file_object.write(\"\\n\")\n",
    "    # Append text at the end of file\n",
    "    file_object.write(text_to_append)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
